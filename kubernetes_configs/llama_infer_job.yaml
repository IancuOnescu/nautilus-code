apiVersion: batch/v1
kind: Job
metadata:
  name: llama3-quick-infer-small
spec:
  template:
    spec:
      # Container that pulls the code
      initContainers:
      - name: install
        image: busybox:1.36
        command: ['wget', 'https://githubraw.com/IancuOnescu/nautilus-code/master/playground_scripts/quick_llama_infer.py', '-O', '/work-dir/quick_llama_infer.py']
        # -------- Storage --------
        volumeMounts:
        - name: workdir
          mountPath: /work-dir
        # -------- Resources --------
        resources:
          limits:
            memory: "2G"
            cpu: "1"
          requests:
            memory: "2G"
            cpu: "1"
      # These are are the main containers
      containers:
        - name: program
          image: huggingface/transformers-pytorch-gpu:latest
          # command: ['python3', '/work-dir/quick_llama_infer.py', '-lf', '/test-data/logs.txt', '-dp', '/test-data/vast_test_3_samples.csv', '-of', '/test-data/output.txt', '-hf', '/test-data/huggingface_token.txt', '-m', '/test-data/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf']
          command: ['tail', '-f', '/dev/null']
          # -------- Storage --------
          volumeMounts:
          - name: iones-persistent-storage
            mountPath: /test-data
          - name: workdir
            mountPath: /work-dir
          # -------- Resources --------
          resources:
            limits:
              nvidia.com/gpu: "1"
              memory: "25G"
              cpu: "3"
            requests:
              nvidia.com/gpu: "1"
              memory: "25G"
              cpu: "3"
      # -------- Storage declaration --------
      volumes:
        - name: iones-persistent-storage
          persistentVolumeClaim:
            claimName: iones-persistent-storage
        - name: workdir
          emptyDir: {}
      restartPolicy: Never
  backoffLimit: 1